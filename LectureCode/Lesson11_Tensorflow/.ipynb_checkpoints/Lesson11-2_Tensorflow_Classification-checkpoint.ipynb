{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson11-2 Tensorflow - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward Propagation\n",
    "### 1. Tensorflow Linear Function\n",
    "Let’s derive the function `y = Wx + b`. We want to translate our input, `x`, to labels, `y`.\n",
    "\n",
    "For example, imagine we want to classify images as digits.\n",
    "\n",
    "`x` would be our list of pixel values, and `y` would be the logits, one for each digit. Let's take a look at `y = Wx`, where the weights, `W`, determine the influence of `x` at predicting each `y`.\n",
    "\n",
    "![](https://video.udacity-data.com/topher/2018/May/5af21f64_wx-1/wx-1.jpg)\n",
    "\n",
    "`y = Wx` allows us to segment the data into their respective labels using a line.\n",
    "\n",
    "However, this line has to pass through the origin, because whenever `x` equals 0, then `y` is also going to equal 0.\n",
    "\n",
    "We want the ability to shift the line away from the origin to fit more complex data. The simplest solution is to add a number to the function, which we call **\"Bias”**.\n",
    "\n",
    "![](https://video.udacity-data.com/topher/2018/May/5af21f8c_wx-b/wx-b.jpg)\n",
    "\n",
    "Our new function becomes `Wx + b`, allowing us to create predictions on linearly separable data. Let’s use a concrete example and calculate the logits.\n",
    "\n",
    "#### Matrix Multiplication\n",
    "Calculate the logits `a` and `b` for the following formula.\n",
    "![](https://video.udacity-data.com/topher/2018/May/5af21fd9_codecogseqn-13/codecogseqn-13.gif)\n",
    "The answer is `a = 0.16`, `b = 0.06`.\n",
    "\n",
    "\n",
    "#### Transposition\n",
    "We've been using the `y = Wx + b` function for our linear function.\n",
    "\n",
    "But there's another function that does the same thing, `y = xW + b`. These functions do the same thing and are interchangeable, except for the dimensions of the matrices involved.\n",
    "\n",
    "To shift from one function to the other, you simply have to swap the row and column dimensions of each matrix. This is called **Transposition**.\n",
    "\n",
    "For rest of this lesson, we actually use xW + b, because this is what TensorFlow uses.\n",
    "\n",
    "![img](https://video.udacity-data.com/topher/2018/May/5af220e2_codecogseqn-18/codecogseqn-18.gif)\n",
    "\n",
    "The above example is identical to the quiz you just completed, except that the matrices are **transposed**.\n",
    "\n",
    "`x` now has the dimensions 1x3, `W` now has the dimensions 3x2, and `b` now has the dimensions `1x2`. Calculating this will produce a matrix with the dimension of `1x2`.\n",
    "\n",
    "You'll notice that the elements in this 1x2 matrix are the same as the elements in the 2x1 matrix from the quiz. Again, these matrices are simply transposed.\n",
    "![](https://video.udacity-data.com/topher/2018/May/5af2210f_codecogseqn-20/codecogseqn-20.gif)\n",
    "\n",
    "We now have our logits! The columns represent the logits for our two labels.   \n",
    "Now you can learn how to train this function in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and Bias in TensorFlow\n",
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out `tf.placeholder()` and `tf.constant()`, since those Tensors can't be modified. This is where `tf.Variable` class comes in.\n",
    "\n",
    "- **tf.Variable()**\n",
    "```python\n",
    "x = tf.Variable(5)\n",
    "```\n",
    "The `tf.Variable` class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. You'll use the `tf.global_variables_initializer()` function to initialize the state of all the Variable tensors.\n",
    "\n",
    "    - Initialization\n",
    "    ```python\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "    ```\n",
    "    The `tf.global_variables_initializer()` call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen.\n",
    "\n",
    "    Initializing the weights with random numbers from a **normal distribution** is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You'll learn more about this in the next lesson, when you study gradient descent.\n",
    "\n",
    "    Similarly, choosing weights from a **normal distribution** prevents any one weight from overwhelming other weights. You'll use the `tf.truncated_normal()` function to generate random numbers from a normal distribution.\n",
    "\n",
    "- **tf.truncated_normal()**\n",
    "\n",
    "```python\n",
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "```\n",
    "The `tf.truncated_normal()` function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.\n",
    "\n",
    "Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias. Let's use the simplest solution, setting the bias to 0.\n",
    "\n",
    "- **tf.zeros()**\n",
    "``` python\n",
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "```\n",
    "The `tf.zeros()` function returns a tensor with all zeros.\n",
    "\n",
    "### Linear Classification Quiz\n",
    "![](https://video.udacity-data.com/topher/2018/May/5af2214a_mnist-012/mnist-012.png)\n",
    "You'll be classifying the handwritten numbers `0`, `1`, and `2` from the **MNIST dataset** using TensorFlow. The above is a small sample of the data you'll be training on. Notice how some of the 1s are written with a [serif](https://en.wikipedia.org/wiki/Serif) at the top and at different angles. The similarities and differences will play a part in shaping the weights of the model.\n",
    "\n",
    "![](https://video.udacity-data.com/topher/2018/May/5af22171_weights-0-1-2/weights-0-1-2.png)\n",
    "Left: Weights for labeling 0.   \n",
    "Middle: Weights for labeling 1.   \n",
    "Right: Weights for labeling 2.   \n",
    "\n",
    "The images above are trained weights for each label (`0`, `1`, and `2`). The weights display the unique properties of each digit they have found. Complete this quiz to train your own weights using the MNIST dataset.\n",
    "\n",
    "- **Instructions**\n",
    "    1. Open quiz.py.\n",
    "        1. Implement get_weights to return a tf.Variable of weights\n",
    "        2. Implement get_biases to return a tf.Variable of biases\n",
    "        3. Implement xW + b in the linear function\n",
    "    2. Open sandbox.py\n",
    "        1. Initialize all weights\n",
    "\n",
    "Since `xW` in `xW + b` is matrix multiplication, you have to use the `tf.matmul()` function instead of `tf.multiply()`. Don't forget that order matters in matrix multiplication, so `tf.matmul(a,b)` is not the same as `tf.matmul(b,a)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Udacity Workspace의 tensorflow version == 1.3.0 이다.\n",
    "## 하지만 현재 받을 수 있는 버전은 tensorflow == 1.15.0이다.\n",
    "## 따라서 Classroom의 코드(현재 아래의 코드)를 그대로 쓰면 오류가 발생하므로 고쳐주자...\n",
    "\n",
    "# Solution is available in the other \"quiz_solution.ipynb\" \n",
    "import tensorflow as tf\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "def linear(input, W, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, W) , b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-a7bbd40658a0>:16: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf1_cpu_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf1_cpu_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf1_cpu_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf1_cpu_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf1_cpu_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"quiz_solution.ipynb\" tab\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from test import *\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.583266258239746\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    # TODO: Initialize session variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Grader\n",
    "\n",
    "To run the grader below, you'll want to run the above training from scratch (if you have otherwise already ran it multiple times). You can reset your kernel and then run all cells for the grader code to appropriately check that you weights and biases achieved the desired end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\박준태\\Desktop\\Workspace\\Udacity\\SDC\\LectureCode\\Lesson11_Tensorflow\\grader_quiz_LinearClassification.py:47: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "Function weights isn't correct.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "### Be sure to run all cells above before running this cell ###\n",
    "import grader_quiz_LinearClassification as grader\n",
    "\n",
    "try:\n",
    "    grader.run_grader(get_weights, get_biases, linear)\n",
    "except Exception as err:\n",
    "    print(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Softmax\n",
    "Congratulations on successfully implementing a linear function that outputs logits. You're one step closer to a working classifier.\n",
    "\n",
    "The next step is to assign a probability to each label, which you can then use to classify the data. Use the softmax function to turn your logits into probabilities.\n",
    "\n",
    "We can do this by using the formula above, which uses the input of y values and the mathematical constant \"e\" which is approximately equal to 2.718. By taking \"e\" to the power of any real value we always get back a positive value, this then helps us scale when having negative y values. The summation symbol on the bottom of the divisor indicates that we add together all the e^(input y value) elements in order to get our calculated probability outputs.\n",
    "\n",
    "![Softmax function](https://video.udacity-data.com/topher/2017/August/59a3b336_softmax/softmax.png)\n",
    "\n",
    "#### Quiz   \n",
    "For the next quiz, you'll implement a `softmax(x)` function that takes in `x`, a one or two dimensional array of logits.\n",
    "\n",
    "In the one dimensional case, the array is just a single set of logits. In the two dimensional case, each column in the array is a set of logits. The `softmax(x)` function should return a NumPy array of the same shape as `x`.\n",
    "\n",
    "For example, given a one-dimensional array:\n",
    "```python\n",
    "# logits is a one-dimensional array with 3 elements\n",
    "logits = [1.0, 2.0, 3.0]\n",
    "# softmax will return a one-dimensional array with 3 elements\n",
    "print softmax(logits)\n",
    "```\n",
    "```\n",
    "$ [ 0.09003057  0.24472847  0.66524096]\n",
    "```\n",
    "   \n",
    "Given a two-dimensional array where each column represents a set of logits:\n",
    "```python\n",
    "# logits is a two-dimensional array\n",
    "logits = np.array([\n",
    "    [1, 2, 3, 6],\n",
    "    [2, 4, 5, 6],\n",
    "    [3, 8, 7, 6]])\n",
    "# softmax will return a two-dimensional array with the same shape\n",
    "print softmax(logits)\n",
    "```\n",
    "```\n",
    "$ [\n",
    "    [ 0.09003057  0.00242826  0.01587624  0.33333333]\n",
    "    [ 0.24472847  0.01794253  0.11731043  0.33333333]\n",
    "    [ 0.66524096  0.97962921  0.86681333  0.33333333]\n",
    "  ]\n",
    "```\n",
    "Implement the softmax function, which is specified by the formula at the top of the page.\n",
    "\n",
    "The probabilities for each column must sum to 1. Feel free to test your function with the inputs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00235563 0.00235563 0.00235563]\n",
      " [0.04731416 0.04731416 0.04731416]\n",
      " [0.95033021 0.95033021 0.95033021]]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # TODO: Compute and return softmax(x)\n",
    "    \n",
    "    arr = np.array(x)\n",
    "    arr_exp = np.exp(arr)\n",
    "    \n",
    "    if arr.ndim == 1 or arr.ndim == 2:\n",
    "        arr_result = arr_exp / np.sum(arr_exp, axis = 0)\n",
    "        return arr_result\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "# logits = [3.0, 1.0, 0.2]\n",
    "logits = np.arange(9).reshape(3, 3)\n",
    "print(softmax(logits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow Softmax\n",
    "Now that you've built a softmax function from scratch, let's see how softmax is done in TensorFlow.\n",
    "```python\n",
    "x = tf.nn.softmax([2.0, 1.0, 0.2])\n",
    "```\n",
    "Easy as that! `tf.nn.softmax()` implements the softmax function for you. It takes in logits and returns softmax activations.\n",
    "\n",
    "#### Quiz\n",
    "Use the softmax function in the quiz below to return the softmax of the logits.\n",
    "![Softmax Quiz](https://video.udacity-data.com/topher/2017/February/58950908_softmax-input-output/softmax-input-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.ipynb\" \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # TODO: Calculate the softmax of the logits\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed in the logit data\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's the correct softmax!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "### Be sure to run all cells above before running this cell ###\n",
    "import grader_quiz_softmax as grader\n",
    "\n",
    "try:\n",
    "    grader.run_grader(run)\n",
    "except Exception as err:\n",
    "    print(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. One-Hot Encoding\n",
    "Classification을 진행할 때, 각각 label들의 확률 중에 제일 높은 것을 1로 하고 나머지를 0으로 만드는 것.\n",
    "$$\n",
    "\\begin{bmatrix} 0.7 \\\\ 0.1 \\\\ 0.2 \\end{bmatrix} → \\begin{bmatrix} 1.0\\\\0.0\\\\0.0 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cross Entropy\n",
    "$S(y)$를 Forward Propagation의 결과라고 하였을 때, $L$은 $S(y)$를 One-Hot Encoding한 것이라고 하자.\n",
    "\n",
    "$$\n",
    "S(y) = \\begin{bmatrix} 0.7 \\\\ 0.1 \\\\ 0.2 \\end{bmatrix},\\quad L = \\begin{bmatrix} 1.0\\\\0.0\\\\0.0 \\end{bmatrix}\n",
    "$$\n",
    "   \n",
    "여기서의 에러를 Cross Entropy로 정의하고 이를 $D(S, L)$이라 할 때,\n",
    "\n",
    "$$\n",
    "D(S, L) = -\\sum_{i} L_{i}log(S_i)\n",
    "$$\n",
    "\n",
    "### 5. 정리 : Forward Propagation\n",
    "\n",
    "![Forward Propagation](Images/ForwardPropagationProcess.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Trainning\n",
    "\n",
    "### 1. Input Data Normalization\n",
    "![Data Normalization](Images/DataNormalization.png)\n",
    "\n",
    "모델 학습을 할 때에는 기본적으로 Gradient Descent를 이용할 것인데, 해당 데이터마다 값의 범위가 0\\~100일 수도 있고, 0\\~1,000,000 일 수도 있고 범위가 다양하다.   \n",
    "\n",
    "만약 해당 데이터들을 그대로 이용하여 Gradient를 구한다면은 데이터 값의 범위마다 큰 쪽으로 편향 될 것이고, 해당 방향에 Learning Rate를 곱한 값으로 학습을 시킨다면 학습효과가 떨어질 것이다.   \n",
    "\n",
    "따라서 데이터 값의 범위를 일정하게 만들어주는 Data Normalization(**Data Standardization**이 더 정확한 표현)을 통해서 학습의 효과를 더욱 높일 수 있다.   \n",
    "\n",
    "강의에서는 Normalization(정규화)라는 표현을 썼지만, 평균과 표준편차를 이용하는 Standardization(표준화)의 수식으로 설명되어 있다. \n",
    "   \n",
    "엄연히 따지자면 Normalization(정규화)와 Standardization(표준화)는 서로 다르다.\n",
    "- 표준화\n",
    "$$\n",
    "    x_{new} = \\frac{x - \\mu}{\\sigma} \\qquad \\mu = average(평균),\\  \\sigma = standard \\ deviation(표준편차)\n",
    "$$   \n",
    "   \n",
    "   \n",
    "- 정규화 : 데이터의 범위를 0과 1사이로 변환하는 것\n",
    "$$\n",
    "    x_{new} = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Weight Initialization\n",
    "모델의 Weight를 초기화 할 때에도 정규분포를 사용하여 초기화하는 것이 학습에 좋다고 한다.(이유는 논문을 찾아봐야할 것 같다.)    \n",
    "   \n",
    "정규분포를 이용하여 Weight을 초기화할 때에 평균과 표준편차를 정해야하는데, 다음과 같은 가이드라인을 주었다.\n",
    "- 평균 = 0\n",
    "- 표준편차 : 표준편차 큰 것이 좋음   \n",
    "    표준편차가 클 수록 평균근처의 값에서 먼 값으로 초기화 될 확률이 높아진다. 따라서 표준편차가 큰 값으로 weight을 초기화하여 Model이 편향되게 초기화되는 것을 막을 수 있으므로, 표준편차가 큰 정규분포를 이용하여 weight을 초기화하는 것이 좋다\n",
    "    ![표준편차별 정규분포](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/1200px-Normal_Distribution_PDF.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Validation Set, Test Set\n",
    "모델을 학습할 때 데이터 셋을 크게 세가지, Tranning Set / Validation Set / Test Set, 로 나눌 수 있다.   \n",
    "\n",
    "- Tranning Set   : 모델을 학습할 때 사용하는 Dataset\n",
    "- Validation Set : 모델을 학습을 검증할 때 사용하는 Dataset\n",
    "- Test Set       : 모델의 학습을 평가할 때 사용하는 Dataset\n",
    "\n",
    "   \n",
    "**`Tranning Set`과 `Test Set`이외에 `Validation Set`은 왜 필요할까?**   \n",
    "훈련의 예를 들어보자. Tranning Set으로 학습을 하고, 여기서 나온 오차값으로 Back Propagation(역전파)를 통하여 모델을 학습시킬 것이다.   \n",
    "   \n",
    "이를 많이 반복할 수록 모델은 점점 `Tranning Set`을 맞출 확률이 올라갈 것이다. 하지만 이를 Test Set을 이용하여 평가해보면 Tranning Set에서 훈련한 정확도보다 떨어진 결과를 얻을 수 있다. 이를 `Overfitting`되었다고 표현한다.   \n",
    "   \n",
    "`Overfitting`이란 쉽게 말해서 Tranning Set에만 모델이 최적화 되고, 다른 Dataset에서는 제대로 찾지 못하는 것을 말한다. 따라서 이러한 Overfitting을 피하기 위하여 Test Set을 분리하여 Test Set의 정확도가 높게 나오는지 평가하는 과정으로 찾는 것이다.   \n",
    "\n",
    "하지만 Test Set의 정확도가 높게 나온다고 하여도 Overfitting을 피했다고 말할 수 있을까? 그렇지 않다. Test Set의 평가를 확인하고 모델의 weight을 다시 학습시키더라도, Test Set에 간접적으로 Overfitting이, 다시 말해 Test Set에 최적화 되도록 간접적으로 학습된다. 따라서 이러한 모델의 학습결과를 검증하는 `Validation Set`을 둬서 학습의 결과를 확인하고, 최종적으로 `Test Set`에서 모델의 학습을 평가하는 과정을 거쳐서 모델의 학습이 Overfitting이 일어나지 않고 잘 되었는지 확인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 여러가지 Gradient Descent 방법과 Learning Rate Decay\n",
    "\n",
    "- SGD(Stochastic Gradient Descent)   \n",
    "    모든 데이터 셋을 한번에 학습하기 힘들어서 만들어진 방법으로, 데이터셋에서 랜덤으로 몇개를 뽑아서 학습시키는 방법이다.    \n",
    "    \n",
    "    모든 데이터셋을 전부 계산하고, 오류값을 계산하여 평균내는 것에는 많은 시간이 걸린다. 따라서 전체 Tranning Set에서 일부를 뽑아서 오류를 계산한 후, 오류를 평균내어 Gradient Descent를 통하여 학습시켜, 마치 전체 데이터셋을 학습시킨 것처럼 진행한다.   \n",
    "    \n",
    "    물론 전체 데이터셋을 하는 것보다 부정확하지만, 작은 batch를 수천번, 수만번 학습시키면 전체데이터셋을 여러번 학습시킨것과 결국 같은 효과를 내게 된다.\n",
    "![SGD](Images/SGD.png)\n",
    "\n",
    "\n",
    "- Momentum   \n",
    "    SGD에서 개선된 방법으로, 이전의 Gradient를 평균내어 이전의 Gradient방향이 관성을 가지는 방법이다.\n",
    "![Momentum](Images/Momentum.png)\n",
    "\n",
    "\n",
    "- Learning Rate Decay   \n",
    "    학습을 여러번 반복할수록 Gradient Descent를 진행할 때, Error 감소율이 점점 떨어진다. 만약 같은 learning rate로 계속 학습을 진행한다면, 일정 Error이하로 떨어지지 않는 것을 볼 수 있다. 이는 일정한 골짜기 높이에서 Gradient가 떨어지지 않고 주위를 뱅뱅 도는 것과 비슷하다. 따라서 좀더 정밀한 정확도를 가지기 위해서는 Learning Rate를 학습이 진행될수록 점점 작게 하는 것이 좋다.\n",
    "![LearningRateDecay](Images/LearningRateDecay.png)   \n",
    "   \n",
    "   \n",
    "* Adagrad : Momentum과 Learning Rate Decay가 적용된 학습방법이다. (자세한건 논문 찾아볼 것...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Mini-batching\n",
    "In this section, you'll go over what mini-batching is and how to apply it in TensorFlow.\n",
    "\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer **lacks the memory to store the entire dataset**.\n",
    "\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch.\n",
    "\n",
    "Let's look at the MNIST dataset with weights and a bias to see if your machine can handle it.\n",
    "\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "```\n",
    "\n",
    "##### Question 1\n",
    "Calculate the memory size of train_features, train_labels, weights, and bias in bytes. Ignore memory for overhead, just calculate the memory required for the stored data.\n",
    "\n",
    "You may have to look up how much memory a float32 requires, using this [link](https://en.wikipedia.org/wiki/Single-precision_floating-point_format).\n",
    "\n",
    "train_features Shape: (55000, 784) Type: float32 → 172.48Mb\n",
    "\n",
    "train_labels Shape: (55000, 10) Type: float32    → 2.2Mb\n",
    "\n",
    "weights Shape: (784, 10) Type: float32           → 31.36Kb\n",
    "\n",
    "bias Shape: (10,) Type: float32                  → 40bytes\n",
    "\n",
    "The total memory space required for the inputs, weights and bias is around 174 megabytes, which isn't that much memory. You could train this whole dataset on most CPUs and GPUs.\n",
    "\n",
    "But larger datasets that you'll use in the future measured in gigabytes or more. It's possible to purchase more memory, but it's expensive. A Titan X GPU with 12 GB of memory costs over $1,000.\n",
    "\n",
    "Instead, in order to run large models on your machine, you'll learn how to use mini-batching.\n",
    "\n",
    "Let's look at how you implement mini-batching in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Mini-batching\n",
    "In order to use mini-batching, you must first divide your data into batches.\n",
    "\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
    "\n",
    "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's `tf.placeholder()` function to receive the varying batch sizes.\n",
    "\n",
    "Continuing the example, if each sample had `n_input = 784` features and `n_classes = 10` possible labels, the dimensions for `features` would be `[None, n_input]` and labels would be `[None, n_classes].\n",
    "\n",
    "```python\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "```\n",
    "What does `None` do here?\n",
    "\n",
    "The `None` dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "\n",
    "Going back to our earlier example, this setup allows you to feed `features` and `labels `into the model as either the batches of 128 samples or the single batch of 104 samples.\n",
    "\n",
    "##### Question 2\n",
    "Use the parameters below, how many batches are there, and what is the last batch size?\n",
    "\n",
    "features is (50000, 400)\n",
    "\n",
    "labels is (50000, 10)\n",
    "\n",
    "batch_size is 128\n",
    "\n",
    "`Answer : batchs = 391, last batch size = 80`\n",
    "\n",
    "##### Question 3\n",
    "Implement the `batches` function to batch `features` and `labels`. The function should return each batch with a maximum size of `batch_size`. To help you with the quiz, look at the following example output of a working `batches` function.\n",
    "```python\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "example_batches = batches(3, example_features, example_labels)\n",
    "```\n",
    "The `example_batches` variable would be the following:\n",
    "```python\n",
    "[\n",
    "    # 2 batches:\n",
    "    #   First is a batch of size 3.\n",
    "    #   Second is a batch of size 1\n",
    "    [\n",
    "        # First Batch is size 3\n",
    "        [\n",
    "            # 3 samples of features.\n",
    "            # There are 4 features per sample.\n",
    "            ['F11', 'F12', 'F13', 'F14'],\n",
    "            ['F21', 'F22', 'F23', 'F24'],\n",
    "            ['F31', 'F32', 'F33', 'F34']\n",
    "        ], [\n",
    "            # 3 samples of labels.\n",
    "            # There are 2 labels per sample.\n",
    "            ['L11', 'L12'],\n",
    "            ['L21', 'L22'],\n",
    "            ['L31', 'L32']\n",
    "        ]\n",
    "    ], [\n",
    "        # Second Batch is size 1.\n",
    "        # Since batch size is 3, there is only one sample left from the 4 samples.\n",
    "        [\n",
    "            # 1 sample of features.\n",
    "            ['F41', 'F42', 'F43', 'F44']\n",
    "        ], [\n",
    "            # 1 sample of labels.\n",
    "            ['L41', 'L42']\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quiz.py\n",
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    \n",
    "    # TODO: Implement batching\n",
    "    result = []\n",
    "    \n",
    "    data_size = len(features)\n",
    "    for idx_start in range(0, data_size, batch_size):\n",
    "        idx_end = idx_start + batch_size\n",
    "        result.append([ features[idx_start:idx_end], labels[idx_start:idx_end] ])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    }
   ],
   "source": [
    "# sandbox.py\n",
    "# from quiz import batches\n",
    "from pprint import pprint\n",
    "\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "# PPrint prints data structures like 2d arrays, so they are easier to read\n",
    "pprint(batches(3, example_features, example_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz : Mini-batch\n",
    "Let's use mini-batching to feed batches of MNIST features and labels into a linear model.\n",
    "\n",
    "Set the batch size and run the optimizer over all the batches with the `batches` function. The recommended batch size is 128. If you have memory restrictions, feel free to make it smaller.\n",
    "\n",
    "This quiz is not graded, see the solution notebook for one way to solve this quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helper import batches\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.13760000467300415\n"
     ]
    }
   ],
   "source": [
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Epochs\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. This section will cover epochs in TensorFlow and how to choose the right number of epochs.\n",
    "\n",
    "The following TensorFlow code trains a model using 10 epochs.\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helper import batches  # Helper function created in Mini-batching section\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "```\n",
    "\n",
    "Running the code will output the following:\n",
    "\n",
    "```\n",
    "Epoch: 0    - Cost: 11.0     Valid Accuracy: 0.204\n",
    "Epoch: 1    - Cost: 9.95     Valid Accuracy: 0.229\n",
    "Epoch: 2    - Cost: 9.18     Valid Accuracy: 0.246\n",
    "Epoch: 3    - Cost: 8.59     Valid Accuracy: 0.264\n",
    "Epoch: 4    - Cost: 8.13     Valid Accuracy: 0.283\n",
    "Epoch: 5    - Cost: 7.77     Valid Accuracy: 0.301\n",
    "Epoch: 6    - Cost: 7.47     Valid Accuracy: 0.316\n",
    "Epoch: 7    - Cost: 7.2      Valid Accuracy: 0.328\n",
    "Epoch: 8    - Cost: 6.96     Valid Accuracy: 0.342\n",
    "Epoch: 9    - Cost: 6.73     Valid Accuracy: 0.36 \n",
    "Test Accuracy: 0.3801000118255615\n",
    "```\n",
    "\n",
    "Each epoch attempts to move to a lower cost, leading to better accuracy.\n",
    "\n",
    "This model continues to improve accuracy up to Epoch 9. Let's increase the number of epochs to 100.\n",
    "\n",
    "```\n",
    "...\n",
    "Epoch: 79   - Cost: 0.111    Valid Accuracy: 0.86\n",
    "Epoch: 80   - Cost: 0.11     Valid Accuracy: 0.869\n",
    "Epoch: 81   - Cost: 0.109    Valid Accuracy: 0.869\n",
    "....\n",
    "Epoch: 85   - Cost: 0.107    Valid Accuracy: 0.869\n",
    "Epoch: 86   - Cost: 0.107    Valid Accuracy: 0.869\n",
    "Epoch: 87   - Cost: 0.106    Valid Accuracy: 0.869\n",
    "Epoch: 88   - Cost: 0.106    Valid Accuracy: 0.869\n",
    "Epoch: 89   - Cost: 0.105    Valid Accuracy: 0.869\n",
    "Epoch: 90   - Cost: 0.105    Valid Accuracy: 0.869\n",
    "Epoch: 91   - Cost: 0.104    Valid Accuracy: 0.869\n",
    "Epoch: 92   - Cost: 0.103    Valid Accuracy: 0.869\n",
    "Epoch: 93   - Cost: 0.103    Valid Accuracy: 0.869\n",
    "Epoch: 94   - Cost: 0.102    Valid Accuracy: 0.869\n",
    "Epoch: 95   - Cost: 0.102    Valid Accuracy: 0.869\n",
    "Epoch: 96   - Cost: 0.101    Valid Accuracy: 0.869\n",
    "Epoch: 97   - Cost: 0.101    Valid Accuracy: 0.869\n",
    "Epoch: 98   - Cost: 0.1      Valid Accuracy: 0.869\n",
    "Epoch: 99   - Cost: 0.1      Valid Accuracy: 0.869\n",
    "Test Accuracy: 0.8696000006198883\n",
    "```\n",
    "\n",
    "From looking at the output above, you can see the model doesn't increase the validation accuracy after epoch 80. Let's see what happens when we increase the learning rate.\n",
    "\n",
    "learn_rate = 0.1\n",
    "\n",
    "```\n",
    "Epoch: 76   - Cost: 0.214    Valid Accuracy: 0.752\n",
    "Epoch: 77   - Cost: 0.21     Valid Accuracy: 0.756\n",
    "Epoch: 78   - Cost: 0.21     Valid Accuracy: 0.756\n",
    "...\n",
    "Epoch: 85   - Cost: 0.207    Valid Accuracy: 0.756\n",
    "Epoch: 86   - Cost: 0.209    Valid Accuracy: 0.756\n",
    "Epoch: 87   - Cost: 0.205    Valid Accuracy: 0.756\n",
    "Epoch: 88   - Cost: 0.208    Valid Accuracy: 0.756\n",
    "Epoch: 89   - Cost: 0.205    Valid Accuracy: 0.756\n",
    "Epoch: 90   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 91   - Cost: 0.207    Valid Accuracy: 0.756\n",
    "Epoch: 92   - Cost: 0.204    Valid Accuracy: 0.756\n",
    "Epoch: 93   - Cost: 0.206    Valid Accuracy: 0.756\n",
    "Epoch: 94   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 95   - Cost: 0.2974   Valid Accuracy: 0.756\n",
    "Epoch: 96   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 97   - Cost: 0.2996   Valid Accuracy: 0.756\n",
    "Epoch: 98   - Cost: 0.203    Valid Accuracy: 0.756\n",
    "Epoch: 99   - Cost: 0.2987   Valid Accuracy: 0.756\n",
    "Test Accuracy: 0.7556000053882599\n",
    "```\n",
    "\n",
    "Looks like the learning rate was increased too much. The final accuracy was lower, and it stopped improving earlier. Let's stick with the previous learning rate, but change the number of epochs to 80.\n",
    "\n",
    "```\n",
    "Epoch: 65   - Cost: 0.122    Valid Accuracy: 0.868\n",
    "Epoch: 66   - Cost: 0.121    Valid Accuracy: 0.868\n",
    "Epoch: 67   - Cost: 0.12     Valid Accuracy: 0.868\n",
    "Epoch: 68   - Cost: 0.119    Valid Accuracy: 0.868\n",
    "Epoch: 69   - Cost: 0.118    Valid Accuracy: 0.868\n",
    "Epoch: 70   - Cost: 0.118    Valid Accuracy: 0.868\n",
    "Epoch: 71   - Cost: 0.117    Valid Accuracy: 0.868\n",
    "Epoch: 72   - Cost: 0.116    Valid Accuracy: 0.868\n",
    "Epoch: 73   - Cost: 0.115    Valid Accuracy: 0.868\n",
    "Epoch: 74   - Cost: 0.115    Valid Accuracy: 0.868\n",
    "Epoch: 75   - Cost: 0.114    Valid Accuracy: 0.868\n",
    "Epoch: 76   - Cost: 0.113    Valid Accuracy: 0.868\n",
    "Epoch: 77   - Cost: 0.113    Valid Accuracy: 0.868\n",
    "Epoch: 78   - Cost: 0.112    Valid Accuracy: 0.868\n",
    "Epoch: 79   - Cost: 0.111    Valid Accuracy: 0.868\n",
    "Epoch: 80   - Cost: 0.111    Valid Accuracy: 0.869\n",
    "Test Accuracy: 0.86909999418258667\n",
    "```\n",
    "\n",
    "The accuracy only reached 0.86, but that could be because the learning rate was too high. Lowering the learning rate would require more epochs, but could ultimately achieve better accuracy.\n",
    "\n",
    "In the upcoming TensorFLow Lab, you'll get the opportunity to choose your own learning rate, epoch count, and batch size to improve the model's accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
