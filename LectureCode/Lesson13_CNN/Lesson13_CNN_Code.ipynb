{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson13 Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convolutional Network in TensorFlow\n",
    "It's time to walk through an example Convolutional Neural Network (CNN) in TensorFlow.\n",
    "\n",
    "The structure of this network follows the classic structure of CNNs, which is a mix of convolutional layers and max pooling, followed by fully-connected layers.\n",
    "\n",
    "The code you'll be looking at is similar to what you saw in the segment on **Deep Neural Network in TensorFlow** in the previous lesson, except we restructured the architecture of this network as a CNN.\n",
    "\n",
    "Just like in that segment, here you'll study the line-by-line breakdown of the code. If you want, you can even [download the code](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a61ca1_cnn/cnn.zip) and run it yourself.\n",
    "\n",
    "Thanks to [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples) for providing the original TensorFlow model on which this segment is based.\n",
    "\n",
    "Time to dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "You've seen this section of code from previous lessons. Here we're importing the MNIST dataset and using a convenient TensorFlow function to batch, scale, and One-Hot encode the data.\n",
    "\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases\n",
    "```python\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Convolutions\n",
    "|<img src = \"https://video.udacity-data.com/topher/2016/November/581a58be_convolution-schematic/convolution-schematic.gif\" width = \"400\"/>|\n",
    "|:---:|\n",
    "|Convolution with 3Ã—3 Filter. (Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution)|\n",
    "\n",
    "The above is an example of a [convolution](https://en.wikipedia.org/wiki/Convolution) with a 3x3 filter and a stride of 1 being applied to data with a range of 0 to 1. The convolution for each 3x3 section is calculated against the weight, `[[1, 0, 1], [0, 1, 0], [1, 0, 1]]`, then a bias is added to create the convolved feature on the right. In this case, the bias is zero. In TensorFlow, this is all done using `tf.nn.conv2d()` and `tf.nn.bias_add()`.\n",
    "\n",
    "```python\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "```\n",
    "\n",
    "The `tf.nn.conv2d()` function computes the convolution against weight W as shown above.\n",
    "\n",
    "In TensorFlow, `strides` is an array of 4 elements; the first element in this array indicates the stride for batch and last element indicates stride for features. It's good practice to remove the batches or features you want to skip from the data set rather than use a stride to skip them. You can always set the first and last element to 1 in `strides` in order to use all batches and features.\n",
    "\n",
    "The middle two elements are the strides for height and width respectively. I've mentioned stride as one number because you usually have a square stride where `height = width`. When someone says they are using a stride of 3, they usually mean `tf.nn.conv2d(x, W, strides=[1, 3, 3, 1])`.\n",
    "\n",
    "To make life easier, the code is using `tf.nn.bias_add()` to add the bias. Using `tf.add()` doesn't work when the tensors aren't the same shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling\n",
    "|<img src = \"https://video.udacity-data.com/topher/2016/November/581a57fe_maxpool/maxpool.jpeg\" width = \"400\"/>|\n",
    "|:---:|\n",
    "|Max Pooling with 2x2 filter and stride of 2. (Source: http://cs231n.github.io/convolutional-networks/)|\n",
    "\n",
    "The above is an example of [max pooling](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) with a 2x2 filter and stride of 2. The left square is the input and the right square is the output. The four 2x2 colors in input represents each time the filter was applied to create the max on the right side. For example, `[[1, 1], [5, 6]]` becomes 6 and `[[3, 2], [1, 2]]` becomes 3.\n",
    "```python\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')\n",
    "```\n",
    "The `tf.nn.max_pool()` function does exactly what you would expect, it performs max pooling with the `ksize` parameter as the size of the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "|<img src = \"https://video.udacity-data.com/topher/2016/November/581a64b7_arch/arch.png\" width = \"400\"/>|\n",
    "|:---:|\n",
    "|Image from Explore The Design Space video|\n",
    "\n",
    "In the code below, we're creating 3 layers alternating between convolutions and max pooling followed by a fully connected and output layer. The transformation of each layer to new dimensions are shown in the comments. For example, the first layer shapes the images from 28x28x1 to 28x28x32 in the convolution step. Then next step applies max pooling, turning each sample into 14x14x32. All the layers are applied from `conv1` to `output`, producing 10 class predictions.\n",
    "\n",
    "```python\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session\n",
    "Now let's run it!\n",
    "\n",
    "```python\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: mnist.validation.images[:test_valid_size],\n",
    "                y: mnist.validation.labels[:test_valid_size],\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images[:test_valid_size],\n",
    "        y: mnist.test.labels[:test_valid_size],\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))\n",
    "```\n",
    "\n",
    "That's it! That is a CNN in TensorFlow. Now that you've seen a CNN in TensorFlow, let's see if you can apply it on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TensorFlow Convolutional Layer Workspaces\n",
    "\n",
    "### Using Convolution Layers in TensorFlow\n",
    "Let's now apply what we've learned to build real CNNs in TensorFlow. In the below exercise, you'll be asked to set up the dimensions of the Convolution filters, the weights, the biases. This is in many ways the trickiest part to using CNNs in TensorFlow. Once you have a sense of how to set up the dimensions of these attributes, applying CNNs will be far more straight forward.\n",
    "\n",
    "### Review\n",
    "You should go over the TensorFlow documentation for [2D convolutions](https://www.tensorflow.org/guide#Convolution). Most of the documentation is straightforward, except perhaps the `padding` argument. The padding might differ depending on whether you pass `'VALID'` or `'SAME'`.\n",
    "\n",
    "Here are a few more things worth reviewing:\n",
    "\n",
    "- Introduction to TensorFlow -> [TensorFlow Variables](https://www.tensorflow.org/guide/variable).\n",
    "- How to determine the dimensions of the output based on the input size and the filter size (shown below). You'll use this to determine what the size of your filter should be.\n",
    "\n",
    "    ```\n",
    "     new_height = (input_height - filter_height + 2 * P)/S + 1\n",
    "     new_width = (input_width - filter_width + 2 * P)/S + 1\n",
    "    ```\n",
    "\n",
    "### Instructions\n",
    "1. Finish off each `TODO` in the `conv2d` function.\n",
    "2. Setup the `strides`, `padding` and filter weight/bias (`F_w` and `F_b`) such that the output shape is `(1, 2, 2, 3)`. Note that all of these except `strides` should be TensorFlow variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(1, 2, 2, 3) dtype=float32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup the strides, padding and filter weight/bias such that\n",
    "the output shape is (1, 2, 2, 3).\n",
    "\"\"\"\n",
    "# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\n",
    "# (1, 4, 4, 1)\n",
    "x = np.array([\n",
    "    [0, 1, 0.5, 10],\n",
    "    [2, 2.5, 1, -8],\n",
    "    [4, 0, 5, 6],\n",
    "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
    "X = tf.constant(x)\n",
    "\n",
    "def conv2d(input_array):\n",
    "    # Filter (weights and bias)\n",
    "    # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "    # The shape of the filter bias is (output_depth,)\n",
    "    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\n",
    "    \n",
    "    ## Solution 1\n",
    "    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\n",
    "    F_W = tf.Variable(tf.random_normal([2, 2, 1, 3]))\n",
    "    F_b = tf.Variable(tf.random_normal([3]))\n",
    "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "    strides = [1, 2, 2, 1]\n",
    "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "    padding = 'VALID'\n",
    "    \n",
    "#     ## Solution 2\n",
    "#     # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\n",
    "#     F_W = tf.Variable(tf.random_normal([3, 3, 1, 3]))\n",
    "#     F_b = tf.Variable(tf.random_normal([3]))\n",
    "#     # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "#     strides = [1, 1, 1, 1]\n",
    "#     # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "#     padding = 'VALID'\n",
    "    \n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\n",
    "    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\n",
    "    return tf.nn.conv2d(input_array, F_W, strides, padding) + F_b\n",
    "\n",
    "output = conv2d(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ë°•ì¤€íƒœ\\Desktop\\Workspace\\Udacity\\SDC\\LectureCode\\Lesson13_CNN\\grader_ConvLayer.py:8: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ë°•ì¤€íƒœ\\Desktop\\Workspace\\Udacity\\SDC\\LectureCode\\Lesson13_CNN\\grader_ConvLayer.py:21: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ë°•ì¤€íƒœ\\Desktop\\Workspace\\Udacity\\SDC\\LectureCode\\Lesson13_CNN\\grader_ConvLayer.py:22: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Great job! Your Convolution layer looks good :)\n"
     ]
    }
   ],
   "source": [
    "##### Do Not Modify ######\n",
    "\n",
    "import grader_ConvLayer as grader\n",
    "\n",
    "test_X = tf.constant(np.random.randn(1, 4, 4, 1), dtype=tf.float32)\n",
    "\n",
    "try:\n",
    "    response = grader.run_grader(test_X, conv2d)\n",
    "    print(response)\n",
    "    \n",
    "    \n",
    "except Exception as err:\n",
    "    print(str(err))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "Here's how I did it.    \n",
    "**NOTE**: there's more than 1 way to get the correct output shape. Your answer might differ from mine.\n",
    "```python\n",
    "def conv2d(input):\n",
    "    # Filter (weights and bias)\n",
    "    F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3)))\n",
    "    F_b = tf.Variable(tf.zeros(3))\n",
    "    strides = [1, 2, 2, 1]\n",
    "    padding = 'VALID'\n",
    "    return tf.nn.conv2d(input, F_W, strides, padding) + F_b\n",
    "```\n",
    "\n",
    "I want to transform the input shape `(1, 4, 4, 1)` to `(1, 2, 2, 3)`. I choose `'VALID'` for the padding algorithm. I find it simpler to understand and it achieves the result I'm looking for.\n",
    "\n",
    "```python\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "```\n",
    "\n",
    "Plugging in the values:\n",
    "\n",
    "```python\n",
    "out_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "out_width  = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "```\n",
    "\n",
    "In order to change the depth from 1 to 3, I have to set the output depth of my filter appropriately:\n",
    "\n",
    "```python\n",
    "F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3))) # (height, width, input_depth, output_depth)\n",
    "F_b = tf.Variable(tf.zeros(3)) # (output_depth)\n",
    "```\n",
    "\n",
    "The input has a depth of 1, so I set that as the `input_depth` of the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorFlow Pooling Layer Workspaces\n",
    "\n",
    "### Using Pooling Layers in TensorFlow\n",
    "In the below exercise, you'll be asked to set up the dimensions of the pooling filters, strides, as well as the appropriate padding. You should go over the TensorFlow documentation for `tf.nn.max_pool()`. Padding works the same as it does for a convolution.\n",
    "\n",
    "### Instructions\n",
    "1. Finish off each `TODO` in the `maxpool` function.\n",
    "2. Setup the `strides`, `padding` and `ksize` such that the output shape after pooling is `(1, 2, 2, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the values to `strides` and `ksize` such that\n",
    "the output shape after pooling is (1, 2, 2, 1).\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)\n",
    "# (1, 4, 4, 1)\n",
    "x = np.array([\n",
    "    [0, 1, 0.5, 10],\n",
    "    [2, 2.5, 1, -8],\n",
    "    [4, 0, 5, 6],\n",
    "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
    "X = tf.constant(x)\n",
    "\n",
    "def maxpool(input):\n",
    "#     ## Solution 1\n",
    "#     # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\n",
    "#     ksize = [1, 2, 2, 1]\n",
    "#     # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "#     strides = [1, 2, 2, 1]\n",
    "#     # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "#     padding = 'VALID'\n",
    "    \n",
    "    ## Solution 2\n",
    "    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\n",
    "    ksize = [1, 3, 3, 1]\n",
    "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "    strides = [1, 1, 1, 1]\n",
    "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "    padding = 'VALID'\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool\n",
    "    return tf.nn.max_pool(input, ksize, strides, padding)\n",
    "    \n",
    "out = maxpool(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Job! Your output shape is: [1, 2, 2, 1]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "### Be sure to run all cells above before running this cell ###\n",
    "import grader_PoolLayer as grader\n",
    "\n",
    "try:\n",
    "    grader.run_grader(out)\n",
    "except Exception as err:\n",
    "    print(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Solution\n",
    "Here's how I did it.   \n",
    "**NOTE**: there's more than 1 way to get the correct output shape. Your answer might differ from mine.\n",
    "\n",
    "```python\n",
    "def maxpool(input):\n",
    "    ksize = [1, 2, 2, 1]\n",
    "    strides = [1, 2, 2, 1]\n",
    "    padding = 'VALID'\n",
    "    return tf.nn.max_pool(input, ksize, strides, padding)\n",
    "```\n",
    "\n",
    "I want to transform the input shape `(1, 4, 4, 1)` to `(1, 2, 2, 1)`. I choose `'VALID'` for the padding algorithm. I find it simpler to understand and it achieves the result I'm looking for.\n",
    "\n",
    "```python\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "```\n",
    "\n",
    "Plugging in the values:\n",
    "\n",
    "```python\n",
    "out_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "out_width  = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "```\n",
    "\n",
    "The depth doesn't change during a pooling operation so I don't have to worry about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lab : LeNet in Tensorflow\n",
    "\n",
    "|<img src = \"https://video.udacity-data.com/topher/2018/June/5b176d06_screenshot-2016-11-26-17.52.14/screenshot-2016-11-26-17.52.14.png\" width = \"500\">|\n",
    "|:---:|\n",
    "|LeNet. (Source: Yann Lecun)|\n",
    "\n",
    "You're now going to put together everything you've learned and implement the [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) architecture using TensorFlow.\n",
    "\n",
    "When you get to your next project, remember that LeNet can be a great starting point for your network architecture!\n",
    "\n",
    "### Instructions\n",
    "1. Go to the LeNet with GPU workspace\n",
    "2. Open the jupyter notebook `LeNet-Lab.ipynb`\n",
    "3. Finish off the architecture implementation in the `LeNet` function. That's the only piece that's missing.\n",
    "\n",
    "**Important*: Remember to **turn off** your GPU when not training.\n",
    "\n",
    "\n",
    "### Preprocessing\n",
    "An MNIST image is initially 784 features (1D). If the data is not normalized from [0, 255] to [0, 1], normalize it. We reshape this to `(28, 28, 1)` (3D), and pad the image with 0s such that the height and width are 32 (centers digit further). Thus, the input shape going into the first convolutional layer is `32x32x1`.\n",
    "\n",
    "### Specs\n",
    "`Convolution layer 1`. The output shape should be `28x28x6`.   \n",
    "\n",
    "`Activation 1`. Your choice of activation function.   \n",
    "\n",
    "`Pooling layer 1`. The output shape should be `14x14x6`.\n",
    "\n",
    "`Convolution layer 2`. The output shape should be `10x10x16`.\n",
    "\n",
    "`Activation 2`. Your choice of activation function.\n",
    "\n",
    "`Pooling layer 2`. The output shape should be `5x5x16`.\n",
    "\n",
    "`Flatten layer`. Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
    "\n",
    "`Fully connected layer 1`. This should have **120 outputs**.\n",
    "\n",
    "`Activation 3`. Your choice of activation function.\n",
    "\n",
    "`Fully connected layer 2`. This should have **84 outputs**.\n",
    "\n",
    "`Activation 4`. Your choice of activation function.\n",
    "\n",
    "`Fully connected layer 3`. This should have **10 outputs**.\n",
    "\n",
    "You'll return the result of the final fully connected layer from the `LeNet` function.\n",
    "\n",
    "If implemented correctly you should see output similar to the following:\n",
    "\n",
    "```\n",
    "EPOCH 1 ...\n",
    "Validation loss = 52.809\n",
    "Validation accuracy = 0.864\n",
    "\n",
    "EPOCH 2 ...\n",
    "Validation loss = 24.749\n",
    "Validation accuracy = 0.915\n",
    "\n",
    "EPOCH 3 ...\n",
    "Validation loss = 17.719\n",
    "Validation accuracy = 0.930\n",
    "\n",
    "EPOCH 4 ...\n",
    "Validation loss = 12.188\n",
    "Validation accuracy = 0.943\n",
    "\n",
    "EPOCH 5 ...\n",
    "Validation loss = 8.935\n",
    "Validation accuracy = 0.954\n",
    "\n",
    "EPOCH 6 ...\n",
    "Validation loss = 7.674\n",
    "Validation accuracy = 0.956\n",
    "\n",
    "EPOCH 7 ...\n",
    "Validation loss = 6.822\n",
    "Validation accuracy = 0.956\n",
    "\n",
    "EPOCH 8 ...\n",
    "Validation loss = 5.451\n",
    "Validation accuracy = 0.961\n",
    "\n",
    "EPOCH 9 ...\n",
    "Validation loss = 4.881\n",
    "Validation accuracy = 0.964\n",
    "\n",
    "EPOCH 10 ...\n",
    "Validation loss = 4.623\n",
    "Validation accuracy = 0.964\n",
    "\n",
    "Test loss = 4.726\n",
    "Test accuracy = 0.962\n",
    "```\n",
    "\n",
    "### Parameters Galore\n",
    "As an additional fun exercise calculate the total number of parameters used by the network. Note, the convolutional layers use weight sharing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
